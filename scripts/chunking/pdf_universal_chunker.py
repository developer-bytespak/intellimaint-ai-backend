#!/usr/bin/env python3
"""
Universal PDF Chunking Pipeline (Embedding-Agnostic v1)
=======================================================

Implements the approved 5-phase chunking strategy for any machine-related PDF:
- Operator manuals
- Warranty/policy manuals  
- Parts catalogs
- Mixed manuals

NO embeddings are generated. Chunks are prepared with `should_embed` flag
for future RAG integration.

Output matches KnowledgeChunk Prisma schema exactly.
"""

import re
import json
import argparse
from dataclasses import dataclass, field
from typing import Optional, List, Tuple
from pathlib import Path


# =============================================================================
# DATA STRUCTURES
# =============================================================================

@dataclass
class ChunkCandidate:
    """Raw chunk before classification"""
    heading: Optional[str]
    content: str
    page_start: Optional[int] = None
    page_end: Optional[int] = None
    image_urls: List[str] = field(default_factory=list)


@dataclass  
class Chunk:
    """
    Final chunk matching KnowledgeChunk Prisma schema:
    
    - id: UUID (generated by DB)
    - source_id: UUID (passed in)
    - chunk_index: INT
    - content: TEXT (clean, readable text only)
    - heading: TEXT?
    - embedding: VECTOR? (NOT generated - future use)
    - token_count: INT?
    - metadata: JSONB?
    - created_at: TIMESTAMP (generated by DB)
    """
    chunk_index: int
    content: str
    heading: Optional[str]
    token_count: int
    metadata: dict
    # embedding: None - not generated in this version
    
    def to_dict(self, source_id: str = None) -> dict:
        """Convert to dictionary for JSON/DB insertion"""
        result = {
            "chunk_index": self.chunk_index,
            "content": self.content,
            "heading": self.heading,
            "token_count": self.token_count,
            "metadata": self.metadata,
            "embedding": None,  # Explicitly null - future use
        }
        if source_id:
            result["source_id"] = source_id
        return result


# =============================================================================
# PHASE 1: UNIVERSAL PREPROCESSING
# =============================================================================

class UniversalPreprocessor:
    """
    Phase 1: Remove noise that applies to ALL documents.
    No document-type-specific logic.
    """
    
    # Patterns to remove entirely
    NOISE_PATTERNS = [
        # Page numbers
        r"^Page\s+\d+\s*$",
        r"^Page\s+\d+\s+of\s+\d+\s*$",
        r"^\d+\s*$",  # Standalone page numbers
        
        # Copyright notices
        r"^Â©.*\d{4}.*$",
        r"^Copyright\s+.*\d{4}.*$",
        
        # Issue numbers / dates alone
        r"^Issue\s+\d+.*$",
        r"^Revision\s+\d+.*$",
        r"^Rev\.\s*\d+.*$",
        
        # Blank page notices
        r"(?i)this page (is )?intentionally (left )?blank",
        
        # Dotted leaders (TOC-style)
        r"\.{5,}",
        
        # Page headers/footers patterns
        r"^[A-Z][a-z]+\s+Manual\s*$",
        r"^Chapter\s+\d+\s*$",
    ]
    
    # TOC/Index detection patterns
    TOC_PATTERNS = [
        r"(?i)^table\s+of\s+contents?\s*$",
        r"(?i)^contents?\s*$",
        r"(?i)^index\s*$",
        r"(?i)^list\s+of\s+figures?\s*$",
        r"(?i)^list\s+of\s+tables?\s*$",
    ]
    
    def __init__(self):
        self.noise_regex = [re.compile(p, re.MULTILINE) for p in self.NOISE_PATTERNS]
        self.toc_regex = [re.compile(p) for p in self.TOC_PATTERNS]
    
    def preprocess(self, text: str) -> str:
        """
        Apply universal preprocessing:
        1. Remove page artifacts
        2. Remove copyright/issue lines
        3. Remove blank page notices
        4. Remove TOC/Index sections
        5. Normalize whitespace
        6. Merge wrapped lines
        """
        lines = text.split("\n")
        cleaned_lines = []
        in_toc_section = False
        
        for line in lines:
            stripped = line.strip()
            
            # Skip empty lines (will normalize later)
            if not stripped:
                cleaned_lines.append("")
                continue
            
            # Check if entering TOC/Index section
            if self._is_toc_heading(stripped):
                in_toc_section = True
                continue
            
            # Exit TOC section on next major heading
            if in_toc_section and self._is_major_heading(stripped):
                in_toc_section = False
            
            # Skip TOC content
            if in_toc_section:
                continue
            
            # Skip noise patterns
            if self._is_noise(stripped):
                continue
            
            # Skip TOC-like lines (heading + page number)
            if self._is_toc_line(stripped):
                continue
            
            cleaned_lines.append(stripped)
        
        # Merge wrapped lines and normalize whitespace
        text = "\n".join(cleaned_lines)
        text = self._normalize_whitespace(text)
        text = self._merge_wrapped_lines(text)
        
        return text.strip()
    
    def _is_noise(self, line: str) -> bool:
        """Check if line matches any noise pattern"""
        for pattern in self.noise_regex:
            if pattern.match(line):
                return True
        return False
    
    def _is_toc_heading(self, line: str) -> bool:
        """Check if line is a TOC/Index heading"""
        for pattern in self.toc_regex:
            if pattern.match(line):
                return True
        return False
    
    def _is_toc_line(self, line: str) -> bool:
        """
        Check if line looks like TOC entry:
        - Heading text followed by page number
        - Contains dotted leaders
        - Page reference patterns
        """
        # Dotted leaders (any amount)
        if "..." in line:
            return True
        
        # ". . . ." spaced dots
        if ". . ." in line:
            return True
        
        # "Heading Name    123" pattern
        if re.match(r"^.{5,50}\s{3,}\d{1,4}\s*$", line):
            return True
        
        # "Something 123" at end of line (page reference)
        if re.match(r"^[A-Za-z].+\s+\d{1,3}\s*$", line) and len(line) < 80:
            return True
        
        return False
    
    def _is_major_heading(self, line: str) -> bool:
        """Check if line is a major heading (exits TOC section)"""
        # ALL CAPS
        if line.isupper() and len(line) > 3:
            return True
        # Numbered heading
        if re.match(r"^\d+\.\s+[A-Z]", line):
            return True
        # Markdown heading
        if line.startswith("#"):
            return True
        return False
    
    def _normalize_whitespace(self, text: str) -> str:
        """Normalize excessive whitespace"""
        # Multiple blank lines -> double newline
        text = re.sub(r"\n\s*\n\s*\n+", "\n\n", text)
        # Multiple spaces -> single space
        text = re.sub(r" {2,}", " ", text)
        return text
    
    def _merge_wrapped_lines(self, text: str) -> str:
        """
        Merge lines that were wrapped in the PDF.
        A line is likely wrapped if:
        - Previous line doesn't end with punctuation
        - Current line starts with lowercase
        """
        lines = text.split("\n")
        merged = []
        
        for i, line in enumerate(lines):
            if not line.strip():
                merged.append(line)
                continue
            
            # Check if should merge with previous
            if merged and merged[-1].strip():
                prev = merged[-1].strip()
                curr = line.strip()
                
                # Merge if previous doesn't end with sentence-ender and current starts lowercase
                if (prev and 
                    prev[-1] not in ".!?:;" and 
                    curr and 
                    curr[0].islower()):
                    merged[-1] = merged[-1].rstrip() + " " + curr
                    continue
            
            merged.append(line)
        
        return "\n".join(merged)


# =============================================================================
# PHASE 2: CHUNK CANDIDATE CREATION
# =============================================================================

class ChunkCandidateCreator:
    """
    Phase 2: Create chunk candidates from preprocessed text.
    Structure-first, style-agnostic approach.
    """
    
    def __init__(self):
        pass
    
    def create_candidates(self, text: str) -> List[ChunkCandidate]:
        """
        Create chunk candidates by:
        1. Splitting on headings
        2. If no headings, split by paragraph blocks
        3. Never split inside step lists, bullet lists, tables
        """
        # Extract and remove images first
        text, image_urls = self._extract_images(text)
        
        # Try heading-based splitting first
        sections = self._split_by_headings(text)
        
        if len(sections) <= 1:
            # No clear headings - split by paragraph blocks
            sections = self._split_by_paragraphs(text)
        
        # Convert to ChunkCandidates
        candidates = []
        for heading, content in sections:
            # Clean junk from content
            content = self._clean_content_junk(content)
            
            # Skip if content is now empty
            if not content or len(content.strip()) < 10:
                continue
            
            # Normalize heading (may return None for noisy headings)
            normalized_heading = self._normalize_heading(heading)
            
            candidates.append(ChunkCandidate(
                heading=normalized_heading,
                content=content.strip(),
                image_urls=image_urls if not candidates else []  # Attach images to first chunk
            ))
        
        return candidates
    
    def _extract_images(self, text: str) -> Tuple[str, List[str]]:
        """
        Extract image URLs to metadata, remove from text.
        Only keep valid URLs.
        """
        image_urls = []
        
        # Markdown images: ![alt](url)
        md_pattern = r"!\[[^\]]*\]\(([^)]+)\)"
        for match in re.finditer(md_pattern, text):
            url = match.group(1)
            if self._is_valid_image_url(url):
                image_urls.append(url)
        
        # Direct image URLs
        url_pattern = r"(https?://[^\s)]+\.(?:png|jpg|jpeg|gif|webp|svg))"
        for match in re.finditer(url_pattern, text, re.IGNORECASE):
            url = match.group(1)
            if self._is_valid_image_url(url):
                image_urls.append(url)
        
        # Remove all image references from text
        text = re.sub(md_pattern, "", text)
        text = re.sub(url_pattern, "", text, flags=re.IGNORECASE)
        
        # Remove broken image placeholders
        text = re.sub(r"\[UPLOAD_FAILED[^\]]*\]", "", text, flags=re.IGNORECASE)
        text = re.sub(r"\[\s*image:\s*\d+[^\]]*\]", "", text, flags=re.IGNORECASE)
        
        return text.strip(), list(set(image_urls))
    
    def _is_valid_image_url(self, url: str) -> bool:
        """Check if URL is a valid image URL (not a placeholder)"""
        if not url:
            return False
        if "UPLOAD_FAILED" in url:
            return False
        if not url.startswith("http"):
            return False
        return True
    
    def _clean_content_junk(self, text: str) -> str:
        """
        Final aggressive cleaning to remove ALL junk from content.
        Apply this to every chunk before storing.
        
        Removes:
        - Copyright lines
        - Page numbers mixed in text
        - "url:" placeholders
        - [TABLE_PLACEHOLDER]
        - Publication metadata lines
        - Issue/revision lines
        """
        lines = text.split("\n")
        cleaned = []
        
        for line in lines:
            stripped = line.strip()
            
            # Skip empty
            if not stripped:
                cleaned.append("")
                continue
            
            # Skip copyright (any variation)
            if re.search(r"copyright|Â©|\(c\)", stripped, re.IGNORECASE):
                continue
            
            # Skip page numbers
            if re.match(r"^Page\s+\d+", stripped, re.IGNORECASE):
                continue
            if re.match(r"^\d+\s*$", stripped):
                continue
            
            # Skip "url:" placeholders (any case, with or without space)
            if re.match(r"^url:\s*", stripped, re.IGNORECASE):
                continue
            
            # Skip lines that are ONLY "url:" anywhere
            if stripped.lower() == "url:":
                continue
            
            # Skip [TABLE_PLACEHOLDER]
            if "[TABLE_PLACEHOLDER]" in stripped:
                continue
            
            # Skip publication metadata
            if re.match(r"^(Issue|Revision|Rev\.|Publication)\s+\w+", stripped, re.IGNORECASE):
                continue
            
            # Skip part numbers alone on a line (e.g., "A040W374")
            if re.match(r"^[A-Z]\d{3,}[A-Z]?\d*\s*$", stripped):
                continue
            
            # Skip effective dates alone
            if re.match(r"^Effective:\s*\d+", stripped, re.IGNORECASE):
                continue
            
            # Skip lines with just punctuation/numbers
            if re.match(r"^[\d\s\-\(\)]+$", stripped) and len(stripped) < 15:
                continue
            
            # Keep the line
            cleaned.append(stripped)
        
        # Rejoin and normalize whitespace
        result = "\n".join(cleaned)
        result = re.sub(r"\n\s*\n\s*\n+", "\n\n", result)
        
        # Remove any "url:" that's embedded in text
        result = re.sub(r"\burl:\s*", "", result, flags=re.IGNORECASE)
        
        return result.strip()
    
    def _split_by_headings(self, text: str) -> List[Tuple[Optional[str], str]]:
        """
        Split text by detected headings.
        
        Heading detection (generic):
        - ALL CAPS or short Title Case
        - Starts with numbering (1, 2.1, A.3)
        - Markdown-style (#, ##)
        - Short line without punctuation followed by paragraphs
        """
        lines = text.split("\n")
        sections = []
        current_heading = None
        current_content = []
        
        for i, line in enumerate(lines):
            stripped = line.strip()
            
            if self._is_heading(stripped, i, lines):
                # Save previous section
                if current_content:
                    content = "\n".join(current_content).strip()
                    if content:
                        sections.append((current_heading, content))
                
                current_heading = stripped
                current_content = []
            else:
                current_content.append(line)
        
        # Save last section
        if current_content:
            content = "\n".join(current_content).strip()
            if content:
                sections.append((current_heading, content))
        
        return sections
    
    def _normalize_heading(self, heading: str) -> Optional[str]:
        """
        Normalize noisy headings.
        Return None if heading is meaningless.
        """
        if not heading:
            return None
        
        heading = heading.strip().lstrip("#").strip()
        
        # Noisy headings to remove
        noisy_patterns = [
            r"^Page$",
            r"^Page\s+\d+",
            r"^\(Illustration\s+#?\d+\)",
            r"^Illustration\s+#?\d+",
            r"^Figure\s+#?\d+$",
            r"^Table\s+#?\d+$",
            r"^English Original Instructions",
            r"^Original Instructions",
        ]
        
        for pattern in noisy_patterns:
            if re.match(pattern, heading, re.IGNORECASE):
                return None
        
        # Too short to be meaningful
        if len(heading) < 3:
            return None
        
        return heading
    
    def _is_heading(self, line: str, index: int, lines: List[str]) -> bool:
        """
        Generic heading detection:
        - ALL CAPS (3+ chars)
        - Short Title Case (< 60 chars, no period at end)
        - Numbered heading (1., 2.1, A.3)
        - Markdown heading (#, ##)
        """
        if not line or len(line) < 2:
            return False
        
        # Markdown headings
        if line.startswith("#"):
            return True
        
        # ALL CAPS (3+ chars, not just abbreviations)
        if line.isupper() and len(line) >= 3 and " " in line:
            return True
        
        # Numbered headings (1., 2.1, A.3, etc.)
        if re.match(r"^(\d+\.)+\s+[A-Z]", line):
            return True
        if re.match(r"^[A-Z]\.\d+\s+", line):
            return True
        
        # Short Title Case without ending punctuation
        if (len(line) < 60 and 
            not line.endswith((".", ",", ";", ":", "!", "?")) and
            line[0].isupper() and
            self._is_title_case(line)):
            # Check if followed by content
            if index + 1 < len(lines) and lines[index + 1].strip():
                return True
        
        return False
    
    def _is_title_case(self, line: str) -> bool:
        """Check if line is in Title Case format"""
        words = line.split()
        if len(words) < 2:
            return False
        
        # Most words should be capitalized
        cap_words = sum(1 for w in words if w[0].isupper())
        return cap_words >= len(words) * 0.6
    
    def _split_by_paragraphs(self, text: str, target_paras: int = 4) -> List[Tuple[Optional[str], str]]:
        """
        Split by paragraph blocks when no headings exist.
        Groups 3-6 paragraphs together.
        """
        paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]
        
        sections = []
        for i in range(0, len(paragraphs), target_paras):
            group = paragraphs[i:i + target_paras]
            content = "\n\n".join(group)
            sections.append((None, content))
        
        return sections


# =============================================================================
# PHASE 3: CONTENT ANALYSIS (Lightweight Heuristics)
# =============================================================================

@dataclass
class ContentSignals:
    """Signals computed for each chunk candidate"""
    word_count: int
    sentence_count: int
    alpha_ratio: float      # % alphabetic characters
    numeric_ratio: float    # % digits
    table_density: float    # indicator of table-like structure
    has_images: bool


class ContentAnalyzer:
    """
    Phase 3: Compute lightweight content signals.
    No ML, just cheap heuristics.
    """
    
    def analyze(self, candidate: ChunkCandidate) -> ContentSignals:
        """Compute content signals for a chunk candidate"""
        text = candidate.content
        
        # Word count
        words = text.split()
        word_count = len(words)
        
        # Sentence count (approximate)
        sentence_count = len(re.findall(r"[.!?]+", text))
        if sentence_count == 0 and word_count > 0:
            sentence_count = 1  # At least one implied sentence
        
        # Character ratios
        total_chars = len(text)
        if total_chars > 0:
            alpha_chars = sum(1 for c in text if c.isalpha())
            digit_chars = sum(1 for c in text if c.isdigit())
            alpha_ratio = alpha_chars / total_chars
            numeric_ratio = digit_chars / total_chars
        else:
            alpha_ratio = 0.0
            numeric_ratio = 0.0
        
        # Table density (repeated rows/columns pattern)
        table_density = self._compute_table_density(text)
        
        # Has images
        has_images = len(candidate.image_urls) > 0
        
        return ContentSignals(
            word_count=word_count,
            sentence_count=sentence_count,
            alpha_ratio=alpha_ratio,
            numeric_ratio=numeric_ratio,
            table_density=table_density,
            has_images=has_images
        )
    
    def _compute_table_density(self, text: str) -> float:
        """
        Estimate table-like structure.
        Higher value = more table-like.
        """
        lines = text.split("\n")
        if len(lines) < 2:
            return 0.0
        
        # Count lines with multiple columns (separated by 3+ spaces or tabs)
        column_lines = 0
        for line in lines:
            if re.search(r"\S+\s{3,}\S+\s{3,}\S+", line):
                column_lines += 1
            elif "\t" in line:
                column_lines += 1
            elif "|" in line:
                column_lines += 1
        
        return column_lines / len(lines)


# =============================================================================
# PHASE 4: CHUNK CLASSIFICATION (should_embed decision)
# =============================================================================

class ChunkClassifier:
    """
    Phase 4: Determine should_embed flag and content_type.
    
    should_embed = true if chunk is "language-heavy":
    - sentence_count >= 2
    - alpha_ratio >= 0.6
    - numeric_ratio <= 0.4
    
    content_type categories:
    - "narrative": language-heavy explanatory content
    - "reference": lookup data, parts lists, SKUs
    - "table": tabular data
    - "image_only": mostly images with minimal text
    """
    
    def classify(self, candidate: ChunkCandidate, signals: ContentSignals) -> Tuple[bool, str]:
        """
        Determine should_embed and content_type.
        
        Returns:
            (should_embed, content_type)
        """
        # Determine content_type first
        content_type = self._determine_content_type(candidate, signals)
        
        # should_embed decision
        should_embed = self._should_embed(signals, content_type)
        
        return should_embed, content_type
    
    def _determine_content_type(self, candidate: ChunkCandidate, signals: ContentSignals) -> str:
        """Determine the content type of the chunk"""
        content = candidate.content.lower()
        
        # Image-only: has images but minimal text
        if signals.has_images and signals.word_count < 20:
            return "image_only"
        
        # Table: high table density
        if signals.table_density >= 0.4:
            return "table"
        
        # Reference: mostly numbers/codes
        if signals.numeric_ratio > 0.3:
            return "reference"
        
        # Reference: TOC-like content (page numbers, dotted leaders)
        if "..." in content or ". . ." in content:
            return "reference"
        
        # Reference: publication metadata
        if "issue" in content and re.search(r"\d{4}", content):
            if signals.word_count < 50:
                return "reference"
        
        # Reference: copyright lines
        if "copyright" in content or "Â©" in content:
            return "reference"
        
        # Reference: very short with no sentences
        if signals.word_count < 30 and signals.sentence_count < 2:
            return "reference"
        
        # Reference: parts list pattern (mostly dashes, numbers)
        if signals.alpha_ratio < 0.5:
            return "reference"
        
        # Narrative ONLY if it has:
        # - Multiple sentences (language flow)
        # - High alpha ratio (actual words, not codes)
        # - Low numeric ratio (not a parts list)
        if (signals.sentence_count >= 2 and 
            signals.alpha_ratio >= 0.65 and 
            signals.numeric_ratio <= 0.3):
            return "narrative"
        
        # Default to reference for ambiguous cases
        return "reference"
    
    def _should_embed(self, signals: ContentSignals, content_type: str) -> bool:
        """
        Determine if chunk should be embedded later.
        
        Embed ONLY if chunk is "language-heavy":
        - sentence_count >= 2
        - alpha_ratio >= 0.6
        - numeric_ratio <= 0.4
        - NOT table/reference/image_only
        """
        # Quick reject for non-narrative content
        if content_type in ("table", "reference", "image_only"):
            return False
        
        # Must have minimum word count
        if signals.word_count < 20:
            return False
        
        # Language-heavy check (strict)
        if (signals.sentence_count >= 2 and
            signals.alpha_ratio >= 0.6 and
            signals.numeric_ratio <= 0.4):
            return True
        
        # Also embed if substantial text even with 1 sentence
        if (signals.word_count >= 50 and 
            signals.alpha_ratio >= 0.7 and
            signals.sentence_count >= 1):
            return True
        
        return False


# =============================================================================
# PHASE 5: CHUNK SIZE CONTROL
# =============================================================================

class ChunkSizeController:
    """
    Phase 5: Enforce chunk size rules.
    
    Target: 150-700 tokens
    - If > 700: split by sub-headings or paragraphs
    - If < 80: merge with neighbor if same heading
    """
    
    def __init__(self, min_tokens: int = 80, max_tokens: int = 700, target_tokens: int = 400):
        self.min_tokens = min_tokens
        self.max_tokens = max_tokens
        self.target_tokens = target_tokens
    
    def estimate_tokens(self, text: str) -> int:
        """Estimate token count (~4 chars per token)"""
        return len(text) // 4
    
    def control_size(self, candidates: List[ChunkCandidate]) -> List[ChunkCandidate]:
        """
        Apply size control:
        1. Split oversized chunks
        2. Merge undersized chunks
        """
        # First pass: split oversized
        sized = []
        for candidate in candidates:
            tokens = self.estimate_tokens(candidate.content)
            
            if tokens > self.max_tokens:
                # Split into smaller chunks
                sub_chunks = self._split_chunk(candidate)
                sized.extend(sub_chunks)
            else:
                sized.append(candidate)
        
        # Second pass: merge undersized
        merged = self._merge_small_chunks(sized)
        
        return merged
    
    def _split_chunk(self, candidate: ChunkCandidate) -> List[ChunkCandidate]:
        """Split oversized chunk by paragraphs"""
        paragraphs = [p.strip() for p in candidate.content.split("\n\n") if p.strip()]
        
        chunks = []
        current_content = []
        current_tokens = 0
        
        for para in paragraphs:
            para_tokens = self.estimate_tokens(para)
            
            if current_tokens + para_tokens > self.max_tokens and current_content:
                # Save current chunk
                chunks.append(ChunkCandidate(
                    heading=candidate.heading,
                    content="\n\n".join(current_content),
                    image_urls=candidate.image_urls if not chunks else []
                ))
                current_content = []
                current_tokens = 0
            
            current_content.append(para)
            current_tokens += para_tokens
        
        # Save last chunk
        if current_content:
            chunks.append(ChunkCandidate(
                heading=candidate.heading,
                content="\n\n".join(current_content),
                image_urls=candidate.image_urls if not chunks else []
            ))
        
        return chunks if chunks else [candidate]
    
    def _merge_small_chunks(self, candidates: List[ChunkCandidate]) -> List[ChunkCandidate]:
        """
        Merge undersized chunks with neighbors.
        Strategy:
        1. Try to merge with next chunk (same heading preferred)
        2. If last or can't merge forward, merge with previous
        3. Repeat until no more small chunks can be merged
        """
        if not candidates:
            return candidates
        
        # Iteratively merge until stable
        changed = True
        while changed:
            changed = False
            merged = []
            i = 0
            
            while i < len(candidates):
                candidate = candidates[i]
                tokens = self.estimate_tokens(candidate.content)
                
                # If chunk is too small, try to merge
                if tokens < self.min_tokens:
                    merged_this_round = False
                    
                    # Try merging with NEXT chunk (prefer same heading)
                    if i + 1 < len(candidates):
                        next_candidate = candidates[i + 1]
                        # Merge with next if same heading OR if both are very small
                        next_tokens = self.estimate_tokens(next_candidate.content)
                        if (candidate.heading == next_candidate.heading or 
                            (tokens < 40 and next_tokens < self.min_tokens)):
                            
                            merged_content = candidate.content + "\n\n" + next_candidate.content
                            merged_images = list(set(candidate.image_urls + next_candidate.image_urls))
                            
                            merged.append(ChunkCandidate(
                                heading=candidate.heading,
                                content=merged_content,
                                image_urls=merged_images
                            ))
                            i += 2  # Skip next
                            changed = True
                            merged_this_round = True
                    
                    # If couldn't merge forward, try merging with PREVIOUS
                    if not merged_this_round and merged:
                        prev_candidate = merged[-1]
                        # Merge with previous if same heading OR if current is very small
                        if candidate.heading == prev_candidate.heading or tokens < 40:
                            # Pop previous and create merged version
                            merged.pop()
                            merged_content = prev_candidate.content + "\n\n" + candidate.content
                            merged_images = list(set(prev_candidate.image_urls + candidate.image_urls))
                            
                            merged.append(ChunkCandidate(
                                heading=prev_candidate.heading,
                                content=merged_content,
                                image_urls=merged_images
                            ))
                            i += 1
                            changed = True
                            merged_this_round = True
                    
                    # If still couldn't merge, keep as-is (edge case)
                    if not merged_this_round:
                        merged.append(candidate)
                        i += 1
                else:
                    # Chunk is large enough, keep it
                    merged.append(candidate)
                    i += 1
            
            # Update candidates for next iteration
            candidates = merged
        
        return merged


# =============================================================================
# FINAL CHUNK BUILDER
# =============================================================================

class FinalChunkBuilder:
    """Build final Chunk objects with proper metadata"""
    
    def __init__(self):
        self.analyzer = ContentAnalyzer()
        self.classifier = ChunkClassifier()
    
    def build_chunks(self, candidates: List[ChunkCandidate]) -> List[Chunk]:
        """Convert candidates to final Chunk objects"""
        chunks = []
        
        for i, candidate in enumerate(candidates):
            # Re-analyze after size control
            signals = self.analyzer.analyze(candidate)
            should_embed, content_type = self.classifier.classify(candidate, signals)
            
            # Estimate tokens
            token_count = len(candidate.content) // 4
            
            # Build metadata (simplified structure)
            metadata = {
                "image_urls": candidate.image_urls,
                "content_type": content_type,
            }
            
            # Add page info if available
            if candidate.page_start is not None:
                metadata["page_start"] = candidate.page_start
            if candidate.page_end is not None:
                metadata["page_end"] = candidate.page_end
            
            chunk = Chunk(
                chunk_index=i,
                content=candidate.content,
                heading=candidate.heading,
                token_count=token_count,
                metadata=metadata
            )
            chunks.append(chunk)
        
        return chunks


# =============================================================================
# MAIN PIPELINE
# =============================================================================

class UniversalChunkingPipeline:
    """
    Complete 5-phase universal chunking pipeline.
    
    Phase 1: Universal preprocessing (noise removal)
    Phase 2: Chunk candidate creation (structure-first)
    Phase 3: Content analysis (lightweight heuristics)
    Phase 4: Chunk classification (should_embed decision)
    Phase 5: Chunk size control (split/merge)
    """
    
    def __init__(self):
        self.preprocessor = UniversalPreprocessor()
        self.candidate_creator = ChunkCandidateCreator()
        self.size_controller = ChunkSizeController()
        self.chunk_builder = FinalChunkBuilder()
    
    def process(self, text: str, source_id: str = None) -> List[Chunk]:
        """
        Process text through all 5 phases.
        
        Args:
            text: Raw markdown/text from PDF
            source_id: UUID of knowledge source (optional)
            
        Returns:
            List of Chunk objects ready for storage
        """
        # Phase 1: Preprocessing
        cleaned = self.preprocessor.preprocess(text)
        
        # Phase 2: Candidate creation
        candidates = self.candidate_creator.create_candidates(cleaned)
        
        # Phase 3 & 4: Analysis and classification happen in builder
        # (Content signals computed per candidate)
        
        # Phase 5: Size control
        sized_candidates = self.size_controller.control_size(candidates)
        
        # Build final chunks
        chunks = self.chunk_builder.build_chunks(sized_candidates)
        
        return chunks
    
    def process_file(self, input_path: str, output_path: str, source_id: str = None) -> dict:
        """
        Process a file and save results.
        
        Args:
            input_path: Path to input markdown file
            output_path: Path to output JSON file
            source_id: UUID of knowledge source
            
        Returns:
            Summary statistics
        """
        # Read input
        with open(input_path, "r", encoding="utf-8") as f:
            text = f.read()
        
        # Process
        chunks = self.process(text, source_id)
        
        # Calculate stats
        total_chars = sum(len(c.content) for c in chunks)
        total_words = sum(len(c.content.split()) for c in chunks)
        total_tokens = sum(c.token_count for c in chunks)
        embeddable = sum(1 for c in chunks if c.metadata.get("content_type") == "narrative")
        
        # Build output
        output = {
            "metadata": {
                "total_chunks": len(chunks),
                "total_chars": total_chars,
                "total_words": total_words,
                "total_tokens": total_tokens,
                "embeddable_chunks": embeddable,
                "reference_chunks": len(chunks) - embeddable,
                "source_id": source_id
            },
            "chunks": [c.to_dict(source_id) for c in chunks]
        }
        
        # Save
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        return output["metadata"]


# =============================================================================
# CLI
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Universal PDF Chunking Pipeline (Embedding-Agnostic)"
    )
    parser.add_argument(
        "--input", "-i",
        required=True,
        help="Input markdown file path"
    )
    parser.add_argument(
        "--output", "-o",
        required=True,
        help="Output JSON file path"
    )
    parser.add_argument(
        "--source-id",
        help="UUID of knowledge source for database insertion"
    )
    
    args = parser.parse_args()
    
    pipeline = UniversalChunkingPipeline()
    stats = pipeline.process_file(args.input, args.output, args.source_id)
    
    print(f"âœ“ Saved {stats['total_chunks']} chunks to {args.output}")
    if args.source_id:
        print(f"  Source ID: {args.source_id}")
    
    print(f"\nðŸ“Š Chunking Summary:")
    print(f"  Input file: {args.input}")
    print(f"  Output file: {args.output}")
    print(f"  Total chunks: {stats['total_chunks']}")
    print(f"  Total characters: {stats['total_chars']}")
    print(f"  Total words: {stats['total_words']}")
    print(f"  Total tokens: {stats['total_tokens']}")
    print(f"  Narrative chunks: {stats['embeddable_chunks']} (language-heavy)")
    print(f"  Reference chunks: {stats['reference_chunks']} (lookup/tables)")


if __name__ == "__main__":
    main()
